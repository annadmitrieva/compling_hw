{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datrie\n",
    "import re\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "rusdict=datrie.Trie.load('russian_dic.trie')\n",
    "\n",
    "r_alphabet = re.compile(u'[0-9а-яА-ЯёЁ]+|[.,:;?!]+')\n",
    "\n",
    "d1=datrie.Trie.load('hyphen_ext.trie')\n",
    "d2=datrie.Trie.load('wrong_hyphen.trie')\n",
    "\n",
    "def lookup(tokens):\n",
    "    s = []\n",
    "    for i in range(len(tokens)):\n",
    "        s.append(tokens[i])\n",
    "        if len(s)>1:\n",
    "            if s[-2].lower()+' '+s[-1].lower() in d1.keys():\n",
    "                word=d1[s[-2].lower()+' '+s[-1].lower()]\n",
    "                s = s[:-2]\n",
    "                s.append(word)\n",
    "    return s     \n",
    "\n",
    "def lookup_non_hyphens(tokens):\n",
    "    s=[]\n",
    "    for i in tokens:\n",
    "        if '-' in i and i in d2.keys():\n",
    "            for w in d2[i].split():\n",
    "                s.append(w)\n",
    "        else:\n",
    "            s.append(i)\n",
    "    return s\n",
    "\n",
    "def assemble_string(tokens):\n",
    "    s=''\n",
    "    for i in range(len(tokens)):\n",
    "        if i==0:\n",
    "            word=tokens[i][0].upper()+tokens[i][1:]\n",
    "            s = s+word\n",
    "        if i>0:\n",
    "            if tokens[i][0] in string.punctuation:\n",
    "                s = s+tokens[i]\n",
    "            else:\n",
    "                s = s + ' ' + tokens[i]\n",
    "    return s\n",
    "\n",
    "def get_prob(s):\n",
    "    prob=0.0\n",
    "    if s in probdict.keys():\n",
    "        prob+=probdict[s]\n",
    "    return prob+0.0001\n",
    "\n",
    "def word_bigrams(word):\n",
    "    if len(word)>0:\n",
    "        word_bigrams=[word[k:k+2] for k in range(len(word)-1)]\n",
    "        word_bigrams.append(word[0])\n",
    "        word_bigrams.append(word[-1])\n",
    "        return word_bigrams\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "def similarity(s1,s2):\n",
    "    v=0\n",
    "    for i in s1:\n",
    "        if i in s2:\n",
    "            s2=s2.replace(i, '')\n",
    "            v+=1\n",
    "    return v\n",
    "\n",
    "def compare(bigrams1, bigrams2):\n",
    "    common=list(set(bigrams1).intersection(bigrams2))\n",
    "    unique=list(set(bigrams1).symmetric_difference(set(bigrams2)))\n",
    "    if len(unique)>0:\n",
    "        coeff=len(common)/len(unique)\n",
    "        return coeff\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def levenshtein(s, t):\n",
    "    if s == t: return 0\n",
    "    elif len(s) == 0: return len(t)\n",
    "    elif len(t) == 0: return len(s)\n",
    "    v0 = [None] * (len(t) + 1)\n",
    "    v1 = [None] * (len(t) + 1)\n",
    "    for i in range(len(v0)):\n",
    "        v0[i] = i\n",
    "    for i in range(len(s)):\n",
    "        v1[0] = i + 1\n",
    "        for j in range(len(t)):\n",
    "            cost = 0 if s[i] == t[j] else 1\n",
    "            v1[j + 1] = min(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost)\n",
    "        for j in range(len(v0)):\n",
    "            v0[j] = v1[j]\n",
    "    return v1[len(t)]\n",
    "\n",
    "def suggest_word(i):\n",
    "    suggest=[]\n",
    "    bigrams_1=word_bigrams(i)\n",
    "    options={}\n",
    "    for j in rusdict.items(i[0]):\n",
    "        if (j[1]>=len(i)-2 and j[1]<=len(i)+2) and similarity(i,j[0])>=int(len(i)/2): \n",
    "            options[j[0]]=compare(bigrams_1,word_bigrams(j[0]))\n",
    "    if options:\n",
    "        max_val=max(options.values())\n",
    "        for i in options.items():\n",
    "            if i[1]==max_val:\n",
    "                suggest.append(i[0])\n",
    "    else:\n",
    "        suggest.append(i)\n",
    "    return suggest\n",
    "\n",
    "probdict = datrie.Trie.load('/media/sf_communalflat/ihatelinguistics/lmodel.trie')\n",
    "char_probdict=datrie.Trie.load('/media/sf_communalflat/ihatelinguistics/lmodel_chars.trie')\n",
    "\n",
    "def count_char_prob(word):\n",
    "    v=0.0\n",
    "    word_bigrams=[word[k:k+2] for k in range(len(word)-1)]\n",
    "    for i in word_bigrams:\n",
    "        if i in char_probdict:\n",
    "            v+=char_probdict[i]\n",
    "    return v\n",
    "                                        \n",
    "def correct_text(sentences):\n",
    "    corrected_text=[]\n",
    "    for i in sentences:\n",
    "        tokens = r_alphabet.findall(i.lower())\n",
    "        tokens1=lookup(tokens)\n",
    "        tokens2=lookup_non_hyphens(tokens1)\n",
    "        sent=[]\n",
    "        for word in tokens2:\n",
    "            if word in rusdict:\n",
    "                sent.append(word)\n",
    "            else:\n",
    "                possible=[]\n",
    "                for i in suggest_word(word):\n",
    "                    possible.append(i)\n",
    "                if len(possible)==1:\n",
    "                    sent.append(possible[0])\n",
    "                elif len(possible)>1:\n",
    "                    probs={}\n",
    "                    lemmas={}\n",
    "                    reverse_lemmas={}\n",
    "                    windows=[]\n",
    "                    for i in possible:\n",
    "                        lemmas[i]=m.lemmatize(i)[0]\n",
    "                        reverse_lemmas[m.lemmatize(i)[0]]=i\n",
    "                        if len(tokens2)==1:\n",
    "                            probs[i]=1-(levenshtein(i, word))\n",
    "                        elif len(tokens2)>=2 and tokens2[0]==word:\n",
    "                            windows.append('pad'+' '+lemmas[i]+' '+m.lemmatize(tokens2[1])[0])\n",
    "                        elif len(tokens2)>=2 and tokens2[-1]==word:\n",
    "                            windows.append(m.lemmatize(tokens2[-2])[0]+' '+lemmas[i]+' '+'pad')\n",
    "                        else:\n",
    "                            windows.append(m.lemmatize(tokens2[tokens2.index(word)-1])[0]+' ' +lemmas[i]+' '+m.lemmatize(tokens2[tokens2.index(word)+1])[0])\n",
    "                    for i in windows:\n",
    "                        window_tokens=i.split()\n",
    "                        probs[reverse_lemmas[window_tokens[1]]]=get_prob('[] []'.format(window_tokens[0],window_tokens[1]))*get_prob('[] []'.format(window_tokens[1],window_tokens[2]))\n",
    "                    max_prob=max(probs.values())\n",
    "                    extra_probs=[]\n",
    "                    for i in probs.items():\n",
    "                        if i[1]==max_prob:\n",
    "                            extra_probs.append(i[0])\n",
    "                    if len(extra_probs)>1:\n",
    "                        extra_dict={}\n",
    "                        for i in extra_probs:\n",
    "                            extra_dict[i]=count_char_prob(i)\n",
    "                        val_l=max(extra_dict, key=extra_dict.get)\n",
    "                        sent.append(val_l)\n",
    "                    else:\n",
    "                        sent.append(extra_probs[0])\n",
    "        corrected_text.append(assemble_string(sent))\n",
    "    return ' '.join(i for i in corrected_text)\n",
    "\n",
    "test=open('/media/sf_communalflat/ihatelinguistics/spell_test.txt', encoding='utf-8').readlines()\n",
    "\n",
    "test_lines=[]\n",
    "i=0\n",
    "while i<=len(test):\n",
    "    test_lines.append(test[i])\n",
    "    i+=3\n",
    "    \n",
    "checked_lines=[]\n",
    "for i in test_lines:\n",
    "    try:\n",
    "        checked_lines.append(correct_text([i]))\n",
    "    except:\n",
    "        checked_lines.append(i)\n",
    "        \n",
    "with open('spelltest_checked7.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in checked_lines:\n",
    "        f.write(i+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
