{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "import re\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "import datrie\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = enchant.Dict(\"ru_RU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_alphabet = re.compile(u'[0-9а-яА-ЯёЁ]+|[.,:;?!]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1=datrie.Trie.load('/media/sf_communalflat/ihatelinguistics/hyphen_ext.trie')\n",
    "d2=datrie.Trie.load('/media/sf_communalflat/ihatelinguistics/wrong_hyphen.trie')\n",
    "\n",
    "def lookup(tokens):\n",
    "    s = []\n",
    "    for i in range(len(tokens)):\n",
    "        s.append(tokens[i])\n",
    "        if len(s)>1:\n",
    "            if s[-2].lower()+' '+s[-1].lower() in d1.keys():\n",
    "                word=d1[s[-2].lower()+' '+s[-1].lower()]\n",
    "                s = s[:-2]\n",
    "                s.append(word)\n",
    "    return s     \n",
    "\n",
    "def lookup_non_hyphens(tokens):\n",
    "    s=[]\n",
    "    for i in tokens:\n",
    "        if '-' in i and i in d2.keys():\n",
    "            for w in d2[i].split():\n",
    "                s.append(w)\n",
    "        else:\n",
    "            s.append(i)\n",
    "    return s\n",
    "\n",
    "def assemble_string(tokens):\n",
    "    s=''\n",
    "    for i in range(len(tokens)):\n",
    "        if i==0:\n",
    "            word=tokens[i][0].upper()+tokens[i][1:]\n",
    "            s = s+word\n",
    "        if i>0:\n",
    "            if tokens[i][0] in string.punctuation:\n",
    "                s = s+tokens[i]\n",
    "            else:\n",
    "                s = s + ' ' + tokens[i]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prob(s):\n",
    "    prob=0.0\n",
    "    if s in probdict.keys():\n",
    "        prob+=probdict[s]\n",
    "    return prob+0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(s1, s2):\n",
    "    v=0\n",
    "    for i in s1:\n",
    "        if i in s2:\n",
    "            s2 = s2.replace(i, '')\n",
    "            v+=1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probdict = datrie.Trie.load('lmodel.trie')\n",
    "\n",
    "def correct_text(sentences):\n",
    "    corrected_text=[]\n",
    "    for i in sentences:\n",
    "        tokens = r_alphabet.findall(i.lower())\n",
    "        tokens1=lookup(tokens)\n",
    "        tokens2=lookup_non_hyphens(tokens1)\n",
    "        sent=[]\n",
    "        for word in tokens2:\n",
    "            if d.check(word)==True:\n",
    "                sent.append(word)\n",
    "            elif d.check(word)==False and not d.suggest(word):\n",
    "                sent.append(word)\n",
    "            elif d.check(word)==False and d.suggest(word):\n",
    "                possible=[]\n",
    "                for i in d.suggest(word):\n",
    "                    possible.append(i)\n",
    "                if len(possible)==1:\n",
    "                    sent.append(possible[0])\n",
    "                elif len(possible)>1:\n",
    "                    probs={}\n",
    "                    lemmas={}\n",
    "                    reverse_lemmas={}\n",
    "                    windows=[]\n",
    "                    for i in possible:\n",
    "                        lemmas[i]=m.lemmatize(i)[0]\n",
    "                        reverse_lemmas[m.lemmatize(i)[0]]=i\n",
    "                        if len(tokens2)==1:\n",
    "                            probs[i]=1-(levenshtein(i, word))\n",
    "                        elif len(tokens2)>=2 and tokens2[0]==word:\n",
    "                            windows.append('pad'+' '+lemmas[i]+' '+m.lemmatize(tokens2[1])[0])\n",
    "                        elif len(tokens2)>=2 and tokens2[-1]==word:\n",
    "                            windows.append(m.lemmatize(tokens2[-2])[0]+' '+lemmas[i]+' '+'pad')\n",
    "                        else:\n",
    "                            windows.append(m.lemmatize(tokens2[tokens2.index(word)-1])[0]+' ' +lemmas[i]+' '+m.lemmatize(tokens2[tokens2.index(word)+1])[0])\n",
    "                    for i in windows:\n",
    "                        window_tokens=i.split()\n",
    "                        probs[reverse_lemmas[window_tokens[1]]]=get_prob('[] []'.format(window_tokens[0],window_tokens[1]))*get_prob('[] []'.format(window_tokens[1],window_tokens[2]))\n",
    "                    max_prob=max(probs, key=probs.get)\n",
    "                    extra_probs=[]\n",
    "                    for i in probs.items():\n",
    "                        if i[1]==probs[max_prob]:\n",
    "                            extra_probs.append(i[0])\n",
    "                    if len(extra_probs)>1:\n",
    "                        extra_dict={}\n",
    "                        for i in extra_probs:\n",
    "                            extra_dict[i]=similarity(i, word)\n",
    "                        val_l=max(extra_dict, key=extra_dict.get)\n",
    "                        sent.append(val_l)\n",
    "                    else:\n",
    "                        sent.append(extra_probs[0])\n",
    "        corrected_text.append(assemble_string(sent))\n",
    "    return ' '.join(i for i in corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=open('/media/sf_communalflat/ihatelinguistics/spell_test.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_lines=[]\n",
    "i=0\n",
    "while i<=len(test):\n",
    "    test_lines.append(test[i])\n",
    "    i+=3\n",
    "with open('spelltest_source.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_lines=[]\n",
    "i=1\n",
    "while i<=len(test):\n",
    "    corr_lines.append(test[i])\n",
    "    i+=3\n",
    "with open('spelltest_answer.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(corr_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checked_lines=[]\n",
    "for i in test_lines:\n",
    "    checked_lines.append(correct_text([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('spelltest_checked1.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in checked_lines:\n",
    "        f.write(i+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
